{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook goes through a necasssary step of cleaning the data before it is used for exploratory data analysis. \n",
    "* The input of this notebook is a training dataset in csv format sourced from Kaggle. https://www.kaggle.com/c/nlp-getting-started\n",
    "* The output of this notebook is a csv file with clean and lemmatized text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re           # regular expression \n",
    "import string       # String Handling\n",
    "import random       #For selecting random rows\n",
    "import nltk         # Natural langauage processing toolkit\n",
    "from nltk.stem import WordNetLemmatizer  #Used for Lemmatizing the text\n",
    "from nltk.corpus import wordnet          #Used for POS tagging \n",
    "from nltk.corpus import stopwords        #Stopwords to be removed from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training data into a dataframe using pandas and viewing the top 20 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 20 rows in 'Keyword' and 'Location' column are NaNs. Now Checking if there are nulls in other columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of unique values in 'keywords' and 'location'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.keyword.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.location.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'locations' column selected randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US, PA',\n",
       " ' Neverland ',\n",
       " 'Maryland ',\n",
       " 'Livonia, MI',\n",
       " 'Medford, NJ',\n",
       " '#MayGodHelpUS',\n",
       " 'Somewhere in China.',\n",
       " 'Na:tinixw / Hoopa, Berkeley',\n",
       " 'Tripsburg, ms.',\n",
       " '?',\n",
       " 'NYC :) Ex- #Islamophobe',\n",
       " 'Worldwide',\n",
       " 'Wanderlust',\n",
       " 'Canada ',\n",
       " \"'SAN ANTONIOOOOO'\",\n",
       " 'New Chicago',\n",
       " 'Dallas Fort-Worth',\n",
       " 'Roads/Trails Everywhere',\n",
       " 'South Korea GMT+9',\n",
       " 'Here & There',\n",
       " 'New York',\n",
       " 'MAURITIUS',\n",
       " 'Boston, Massachusetts',\n",
       " 'Chicago, IL 60607',\n",
       " 'Hartford  London Hong Kong',\n",
       " 'The Canopy Kingdom',\n",
       " 'Definitely NOT the stables',\n",
       " 'University of South Florida',\n",
       " 'Helsinki, Finland',\n",
       " 'Arundel ']"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.location.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'keywords' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['burning',\n",
       " 'suicide%20bomb',\n",
       " 'crushed',\n",
       " 'flattened',\n",
       " 'drought',\n",
       " 'radiation%20emergency',\n",
       " 'survivors',\n",
       " 'survived',\n",
       " 'sinking',\n",
       " 'threat',\n",
       " 'mass%20murder',\n",
       " 'floods',\n",
       " 'typhoon',\n",
       " 'crashed',\n",
       " 'blown%20up',\n",
       " 'outbreak',\n",
       " 'curfew',\n",
       " 'eyewitness',\n",
       " 'snowstorm',\n",
       " 'chemical%20emergency',\n",
       " 'windstorm',\n",
       " 'hazardous',\n",
       " 'fatalities',\n",
       " 'mass%20murderer',\n",
       " 'detonate',\n",
       " 'landslide',\n",
       " 'dust%20storm',\n",
       " 'wild%20fires',\n",
       " 'rescue',\n",
       " 'storm']"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.keyword.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'text' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Doing Giveaway Music Kit Dren Death's Head Demolition: http://t.co/fHKhCqPl7j\",\n",
       " \"There's a big storm brewing dark clouds thunder and rain carrying thick dust. This could be interesting.\",\n",
       " 'Read a Schoolboy\\x89Ûªs Eyewitness Account of Hiroshima http://t.co/pq0D7MH3qr',\n",
       " \"I'm drowning in hw now and that's w/o going to swim ohlordy\",\n",
       " \"@lauren_miller_7 she won't harm you\",\n",
       " \"You can never escape me. Bullets don't harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.\",\n",
       " \"PRAYERS FOR MY COUSIN! He's in California helping with the wild fires.\",\n",
       " \"@WeLoveRobDyrdek @adrian_peel not a damn clue. Tried to see how many clicks I could get before I blew up. That's how I played.\",\n",
       " 'Nice job Calgary transit ! http://t.co/RGOGUyt0LF',\n",
       " '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       " 'Hiroshima: 70 years since the worst mass murder in human history. Never forget. http://t.co/jLu2J5QS8U',\n",
       " '@Welles_7 he was injured. He is a pro bowl back.',\n",
       " \"Thanks a lot roadworks men cos a tube strike wasn't disruptive enough so having to walk the desolate route from Tottenham to .....\",\n",
       " '.@CityofCalgary activates emergency plan amid severe thunderstorm warning http://t.co/pc7S8NxJ6Q #yyc #abstorm http://t.co/9xoHmMlMDY',\n",
       " 'HTML5 Beginners Crash Course http://t.co/Y32oWBroVF #course http://t.co/Vr2U4cErW8',\n",
       " 'Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the spirit the angel took me to the top of an enormous high mountain and... http://t.co/v8AfTD9zeZ',\n",
       " '1.9 #Earthquake in 5Km S Of Volcano Hawaii #iPhone users download the Earthquake app for more information http://t.co/V3aZWOAmzK',\n",
       " '@petereallen @HuffPostUK @bbc5live How significant do you think the iconic pic of Church (maybe) ruins and less substantial efforts razed?',\n",
       " '@NickLee8  i went to school in a bombed out East End of London3 families to one house no bathroom outside loo &amp; poor so whats yr point',\n",
       " 'Plane debris discovered on Reunion Island belongs to flight MH370 \\x89ÛÒ Malaysian PM http://t.co/jkc0DIqvXC',\n",
       " \"Next Man Up---AH SCREW THIS! I'm so tired of injuries.  \\n\\nWhat happened to Camp Cupcake? More like Camp Cramp and Break.\",\n",
       " \"Holmgren: We referred to those 35 days as 'the hostage situation.' We were Lou's 'hostages.'\",\n",
       " \"A new tropical cyclone is forming near Guam.\\n\\nOnce it is formed it will be called 'Molave'.\",\n",
       " 'Orchid - Sign Of The Witch http://t.co/YtkXwPyIHg',\n",
       " '#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/J2aQs5loxu #prebreak #best',\n",
       " \"I added a video to a @YouTube playlist http://t.co/1vjAlJA1SX GTA 5 Funny Moments - 'OBLITERATION!' (GTA 5 Online Funny Moments)\",\n",
       " 'Learn How I Gained Access To The Secrets Of The Top Earners &amp; Used Them To Explode My Home Business Here: http://t.co/SGXP1U5OL1 Please #RT',\n",
       " 'Madhya Pradesh Train Derailment: Village Youth Saved Many Lives',\n",
       " 'Police Officer Wounded Suspect Dead After Exchanging Shots',\n",
       " '@Drsarwatzaib070 come on. IK will face MCourt for attacking parliment and hijacking TV station.']"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting 30 random from text column randomly\n",
    "random.sample(list((train_df.text)),k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data incosistencies or redundant information found in the dataset are as follows\n",
    "\n",
    "* Upper case and lower case at unexpected location\n",
    "* Punctuations\n",
    "* Numbers in text \n",
    "* Use of cities, states and Country names. (Granularity problem)\n",
    "* Special characters such as \\x89ÛÒ and \\n\n",
    "* Hyperklinks\n",
    "* Tags in tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'location' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* Removes texts with numbers\n",
    "* Removes cities names if country/state names are mentioned. (High level granularity is maintained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to chaange text to lower case and remove punctionation\n",
    "def cleaning_location(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    #lower case\n",
    "    text = text.split(',')[-1:][0].strip() # Removing city names when country/state name is present\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  #removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #removing text with number\n",
    "    return text\n",
    "\n",
    "# Applying the method to location column\n",
    "train_df.location = train_df.location.apply(cleaning_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'keyword' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* replaces number with space (as %20 was found in middle of two words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_keyword(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('(\\d+)', ' ', text)   #replacing numbers with space\n",
    "    return text\n",
    "\n",
    "# Applying the method to keyword column\n",
    "train_df.keyword = train_df.keyword.apply(cleaning_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method to clean anomalies in 'text' column does the following\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes words starting with @ to remove the tags and mentions example: @barackobama\n",
    "* Adds a column with hashtag values\n",
    "* Removes links\n",
    "* Removes punctuation\n",
    "* Removes words with numbers\n",
    "* Removes special characters examples: \\x89û,\\x89ûó etc \n",
    "* Removes '\\n' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    \n",
    "    text = re.sub(r'@[A-Za-z]+[A-Za-z0-9-_]+', '',text) #removing any word starting with @   \\w\n",
    "    text = re.sub(r'https|www|http\\S+', '', text)  #removing any word starting with http\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  #removing words with numbers\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # removing special characters\n",
    "    text.replace(\"\\n\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts NLTK tags to wordnet tags which would be used to lemmatize the words in the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method is used to lemmatize the sentences in the following order:\n",
    "\n",
    "* Clean the sentences by calling the cleaning_text method on each sentence.\n",
    "* Tokenizing the sentence and generating a nltk POS tag for each word in the cleaned sentence.\n",
    "* Converting the nltk POS tag to Wordnet POS tag by calling wordnet_tag method.\n",
    "* Removes the stopwords\n",
    "* Lemmatizes the tokens using the pos tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence = cleaning_text(sentence) #Cleaning the sentence\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  #Tokenizing and tagging each word \n",
    "    wordnet_tagged = map(lambda x: (x[0], wordnet_tag(x[1])), nltk_tagged)  # Coverting the NLTK tags to wordnet tag\n",
    "    \n",
    "    #Lemmatizing the tagged tokens\n",
    "    lemmatized_sentence = [] #empty list for lemmatized words\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if word not in set(stopwords.words('english')):  #removing stopwords\n",
    "            if tag is None:                   \n",
    "                lemmatized_sentence.append(word) #adding the word as it is if POS tag missing\n",
    "            else:        \n",
    "                #else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizing.lemmatize(word, tag))  ##lemmatizing the token using the POS tag\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the lemmatizing method to text column \n",
    "train_df.text= train_df.text.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7182</td>\n",
       "      <td>10291</td>\n",
       "      <td>weapon</td>\n",
       "      <td>nan</td>\n",
       "      <td>nothing wrong lethal weapon series great yes t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7440</td>\n",
       "      <td>10647</td>\n",
       "      <td>wounds</td>\n",
       "      <td>kashmir</td>\n",
       "      <td>acc study conduct skim morethan population kas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3903</td>\n",
       "      <td>5553</td>\n",
       "      <td>flattened</td>\n",
       "      <td>delhi</td>\n",
       "      <td>flatten frog</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6382</td>\n",
       "      <td>9120</td>\n",
       "      <td>suicide bomb</td>\n",
       "      <td>lagos unilag</td>\n",
       "      <td>old pkk suicide bomber detonate bomb turkey ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>1518</td>\n",
       "      <td>body bags</td>\n",
       "      <td>tx</td>\n",
       "      <td>shoot shit till see body bag</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1133</td>\n",
       "      <td>1633</td>\n",
       "      <td>bombing</td>\n",
       "      <td>nan</td>\n",
       "      <td>cryptic word guide pilot hiroshima bomb mission</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1374</td>\n",
       "      <td>1981</td>\n",
       "      <td>bush fires</td>\n",
       "      <td>nan</td>\n",
       "      <td>ted cruz fire back jeb amp bush lose republica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>465</td>\n",
       "      <td>armageddon</td>\n",
       "      <td>united states</td>\n",
       "      <td>pbban fighterdena armageddon kill flag fast xp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2089</td>\n",
       "      <td>3003</td>\n",
       "      <td>death</td>\n",
       "      <td>nan</td>\n",
       "      <td>call text niggas bff amp boyfriend love boy de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1183</td>\n",
       "      <td>1703</td>\n",
       "      <td>bridge collapse</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>urgentthere currently storey building church b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2087</td>\n",
       "      <td>3000</td>\n",
       "      <td>dead</td>\n",
       "      <td>united states</td>\n",
       "      <td>wyrmwood road dead fuck awesome awesome end aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>4436</td>\n",
       "      <td>electrocute</td>\n",
       "      <td>london</td>\n",
       "      <td>seriously electrocute half uk army touch bangt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>7935</td>\n",
       "      <td>rainstorm</td>\n",
       "      <td>nan</td>\n",
       "      <td>rainstorm update north south</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4866</td>\n",
       "      <td>6929</td>\n",
       "      <td>mass murderer</td>\n",
       "      <td>fresno</td>\n",
       "      <td>happy boy mass murderer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5489</td>\n",
       "      <td>7832</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>vìa lìáctea</td>\n",
       "      <td>reddit quarantine offensive content reddit cof...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5947</td>\n",
       "      <td>8496</td>\n",
       "      <td>screaming</td>\n",
       "      <td>nan</td>\n",
       "      <td>quiet enough literally hear phandom scream tyl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3182</td>\n",
       "      <td>4568</td>\n",
       "      <td>emergency plan</td>\n",
       "      <td>canada</td>\n",
       "      <td>city activate municipal emergency plan primari...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7548</td>\n",
       "      <td>10789</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>four hundred wrecked car cost apiece purchase ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3994</td>\n",
       "      <td>5673</td>\n",
       "      <td>floods</td>\n",
       "      <td>london</td>\n",
       "      <td>there person amp reckon youre die brain flood ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>3336</td>\n",
       "      <td>demolished</td>\n",
       "      <td>ireland</td>\n",
       "      <td>suite office come salvis bistro site former sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6746</td>\n",
       "      <td>9665</td>\n",
       "      <td>tornado</td>\n",
       "      <td>nan</td>\n",
       "      <td>lily xo sexy cowgirl stick view download video</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4809</td>\n",
       "      <td>6845</td>\n",
       "      <td>loud bang</td>\n",
       "      <td>nan</td>\n",
       "      <td>need work office bang fav future jam loud</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4398</td>\n",
       "      <td>6254</td>\n",
       "      <td>hijacking</td>\n",
       "      <td>greece</td>\n",
       "      <td>murderous story america first hijack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>3030</td>\n",
       "      <td>death</td>\n",
       "      <td>nan</td>\n",
       "      <td>katherines death</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3128</td>\n",
       "      <td>4492</td>\n",
       "      <td>electrocuted</td>\n",
       "      <td>nan</td>\n",
       "      <td>elsa gon na end get electrocute shes gon na en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>469</td>\n",
       "      <td>armageddon</td>\n",
       "      <td></td>\n",
       "      <td>armageddon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1954</td>\n",
       "      <td>2813</td>\n",
       "      <td>cyclone</td>\n",
       "      <td>republic of the philippines</td>\n",
       "      <td>new tropical cyclone form near guam form call ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7428</td>\n",
       "      <td>10626</td>\n",
       "      <td>wounded</td>\n",
       "      <td>virginia</td>\n",
       "      <td>serve veteran oifoef vet physical wound many i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>1393</td>\n",
       "      <td>body bag</td>\n",
       "      <td>nan</td>\n",
       "      <td>new summer long thin body bag hip word skirt blue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4044</td>\n",
       "      <td>5744</td>\n",
       "      <td>forest fires</td>\n",
       "      <td>nan</td>\n",
       "      <td>little concerned number forest fire ill live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id          keyword                     location  \\\n",
       "7182  10291           weapon                          nan   \n",
       "7440  10647           wounds                      kashmir   \n",
       "3903   5553        flattened                        delhi   \n",
       "6382   9120     suicide bomb                 lagos unilag   \n",
       "1049   1518        body bags                           tx   \n",
       "1133   1633          bombing                          nan   \n",
       "1374   1981       bush fires                          nan   \n",
       "319     465       armageddon                united states   \n",
       "2089   3003            death                          nan   \n",
       "1183   1703  bridge collapse                      nigeria   \n",
       "2087   3000             dead                united states   \n",
       "3090   4436      electrocute                       london   \n",
       "5560   7935        rainstorm                          nan   \n",
       "4866   6929    mass murderer                       fresno   \n",
       "5489   7832       quarantine                  vìa lìáctea   \n",
       "5947   8496        screaming                          nan   \n",
       "3182   4568   emergency plan                       canada   \n",
       "7548  10789          wrecked                 pennsylvania   \n",
       "3994   5673           floods                       london   \n",
       "2320   3336       demolished                      ireland   \n",
       "6746   9665          tornado                          nan   \n",
       "4809   6845        loud bang                          nan   \n",
       "4398   6254        hijacking                       greece   \n",
       "2110   3030            death                          nan   \n",
       "3128   4492     electrocuted                          nan   \n",
       "323     469       armageddon                                \n",
       "1954   2813          cyclone  republic of the philippines   \n",
       "7428  10626          wounded                     virginia   \n",
       "963    1393         body bag                          nan   \n",
       "4044   5744     forest fires                          nan   \n",
       "\n",
       "                                                   text  target  \n",
       "7182  nothing wrong lethal weapon series great yes t...       0  \n",
       "7440  acc study conduct skim morethan population kas...       1  \n",
       "3903                                       flatten frog       0  \n",
       "6382  old pkk suicide bomber detonate bomb turkey ar...       1  \n",
       "1049                       shoot shit till see body bag       0  \n",
       "1133    cryptic word guide pilot hiroshima bomb mission       1  \n",
       "1374  ted cruz fire back jeb amp bush lose republica...       0  \n",
       "319   pbban fighterdena armageddon kill flag fast xp...       0  \n",
       "2089  call text niggas bff amp boyfriend love boy de...       0  \n",
       "1183  urgentthere currently storey building church b...       1  \n",
       "2087  wyrmwood road dead fuck awesome awesome end aw...       0  \n",
       "3090  seriously electrocute half uk army touch bangt...       0  \n",
       "5560                       rainstorm update north south       1  \n",
       "4866                            happy boy mass murderer       1  \n",
       "5489  reddit quarantine offensive content reddit cof...       0  \n",
       "5947  quiet enough literally hear phandom scream tyl...       0  \n",
       "3182  city activate municipal emergency plan primari...       1  \n",
       "7548  four hundred wrecked car cost apiece purchase ...       0  \n",
       "3994  there person amp reckon youre die brain flood ...       0  \n",
       "2320  suite office come salvis bistro site former sa...       0  \n",
       "6746     lily xo sexy cowgirl stick view download video       0  \n",
       "4809          need work office bang fav future jam loud       0  \n",
       "4398               murderous story america first hijack       1  \n",
       "2110                                   katherines death       0  \n",
       "3128  elsa gon na end get electrocute shes gon na en...       0  \n",
       "323                                          armageddon       0  \n",
       "1954  new tropical cyclone form near guam form call ...       1  \n",
       "7428  serve veteran oifoef vet physical wound many i...       1  \n",
       "963   new summer long thin body bag hip word skirt blue       0  \n",
       "4044       little concerned number forest fire ill live       1  "
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the cleaned dataframe as csv to be used for Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"Clean_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning test data using the predefined methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the method to location column\n",
    "test_df.location = test_df.location.apply(cleaning_location)\n",
    "# Applying the method to keyword column\n",
    "test_df.keyword = test_df.keyword.apply(cleaning_keyword)\n",
    "\n",
    "# Applying the lemmatizing method to text column \n",
    "test_df.text= test_df.text.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('Clean_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
