{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook goes through a necasssary step of cleaning the data before it is used for exploratory data analysis. \n",
    "* The input of this notebook is a training dataset in csv format sourced from Kaggle. \n",
    "* The output of this notebook is a Term frequency-inverse document frequency(TF-IDF) Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re           # regular expression \n",
    "import string       # String Handling\n",
    "import random       #For selecting random rows\n",
    "import nltk         # Natural langauage processing toolkit\n",
    "from nltk.stem import WordNetLemmatizer  #Used for Lemmatizing the text\n",
    "from nltk.corpus import wordnet          #Used for POS tagging \n",
    "from nltk.corpus import stopwords        #Stopwords to be removed from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training data into a dataframe using pandas and viewing the top 20 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 20 rows in 'Keyword' and 'Location' column are NaNs. Now Checking if there are nulls in other columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of unique values in 'keywords' and 'location'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.keyword.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.location.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 50 rows from the 'locations' column selected randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Georgia ? Tennessee',\n",
       " 'San Francisco, CA',\n",
       " 'Birmingham',\n",
       " 'Pig Symbol, Alabama',\n",
       " 'Nantes, France',\n",
       " 'Oakland, CA',\n",
       " 'eritrean',\n",
       " 'Fountain City, IN ',\n",
       " '?????????????, Thailand ',\n",
       " 'Cumming, GA',\n",
       " 'Lyallpur, Pakistan',\n",
       " 'Reddit',\n",
       " 'Burlington, VT',\n",
       " 'Waialua, Hawaii',\n",
       " 'Madison, Wisconsin, USA',\n",
       " 'Up a hill',\n",
       " '36 & 38',\n",
       " 'WESTSIDE OF PHILLY 7? BLOCK??',\n",
       " 'louisville, kentucky',\n",
       " 'rowyso dallas ',\n",
       " 'University of Chicago',\n",
       " 'MAD as Hell',\n",
       " 'Groton, CT',\n",
       " 'Gwersyllt, Wales',\n",
       " 'Holland MI via Houston, CLE',\n",
       " 'Portoviejo-Manabi-Ecuador',\n",
       " 'MA via PA',\n",
       " 'BILASPUR,CHHATTISGARH,495001',\n",
       " 'Alexandria, VA, USA',\n",
       " 'miami x dallas ',\n",
       " 'State of Georgia',\n",
       " 'U.K.',\n",
       " 'Orlando/Cocoa Beach, FL',\n",
       " 'NYHC',\n",
       " 'Playa',\n",
       " 'Bucks County, Pa',\n",
       " 'Spain',\n",
       " 'Instagram: trillrebel_',\n",
       " 'Indonesia',\n",
       " 'in the Word of God',\n",
       " 'Atlanta,Ga',\n",
       " \"The Sun's Corona\",\n",
       " 'Toronto, Bob-Lo, Miami Beach',\n",
       " 'Warri',\n",
       " 'SaudI arabia - riyadh ',\n",
       " 'http://twitch.tv/jcmonkey',\n",
       " 'NYC',\n",
       " 'Ames, Iowa',\n",
       " 'Australia ',\n",
       " 'Here.']"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.location.unique())),k=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 50 rows from the 'keywords' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wounds',\n",
       " 'ablaze',\n",
       " 'deluged',\n",
       " 'volcano',\n",
       " 'demolition',\n",
       " 'devastated',\n",
       " 'inundation',\n",
       " 'casualties',\n",
       " 'obliterated',\n",
       " 'destruction',\n",
       " 'wild%20fires',\n",
       " 'curfew',\n",
       " 'burned',\n",
       " 'fatality',\n",
       " 'flood',\n",
       " 'demolished',\n",
       " 'avalanche',\n",
       " 'rioting',\n",
       " 'deaths',\n",
       " 'annihilated',\n",
       " 'sirens',\n",
       " 'ambulance',\n",
       " 'lava',\n",
       " 'body%20bag',\n",
       " 'suicide%20bomber',\n",
       " 'sunk',\n",
       " 'drowned',\n",
       " 'bush%20fires',\n",
       " 'wrecked',\n",
       " 'flames',\n",
       " 'exploded',\n",
       " 'war%20zone',\n",
       " 'storm',\n",
       " 'meltdown',\n",
       " 'massacre',\n",
       " 'casualty',\n",
       " 'thunderstorm',\n",
       " nan,\n",
       " 'windstorm',\n",
       " 'destroy',\n",
       " 'twister',\n",
       " 'forest%20fire',\n",
       " 'wreckage',\n",
       " 'bridge%20collapse',\n",
       " 'screamed',\n",
       " 'bombing',\n",
       " 'flattened',\n",
       " 'trapped',\n",
       " 'terrorist',\n",
       " 'hostage']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.keyword.unique())),k=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'text' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Something Frightening is Happening to the Weather in the Middle East http://t.co/bDOTQ8dSlN',\n",
       " 'ITS A TIE DYE EXPLOSION ON IG HELP ME. IM DROWNING IN TIE DYE',\n",
       " 'Your PSA for the day: If a fire truck is behind you with lights going MOVE OVER!!! so they can get to their call.',\n",
       " 'Aug. 06 2015 Radio Show articles \\x89ÛÒ  \\n1] Eyewitness Account of Abortion Organ Harvesting by Planned Parenthood: http://t.co/49izkbOHri',\n",
       " 'WWE 2k15 MyCareer EP18 Tyrone body bagging dudes: http://t.co/mr5bI4KD82 via @YouTube',\n",
       " 'Ah yes the gays are totally destroying America. I can see buildings burning and meteors crashing into schools wow',\n",
       " 'Washington Post - 4 dead dozens injured in Gaza blast near house leveled in summer war http://t.co/VjXa13n8Ap',\n",
       " \"But put 'Flood of Fire' at the top of the list.  https://t.co/p5JPjgiipW\",\n",
       " 'Consent Order on cleanup underway at CSX derailment site - Knoxville News Sentinel http://t.co/GieSoMgWTR http://t.co/NMFsgKf1Za',\n",
       " \"Am now repped by the fantastic Laura Milne @TheJonesesVoice for all your liguistic needs. And that's some tongue twister tweets\",\n",
       " '\\x89ÛÏA voluntary evacuation is being recommended at this time\\x89Û\\x9d for Pickerel Lake cabins across highway from #Reidlake fire says MACA #NWT #YZF',\n",
       " 'Truth...\\nhttps://t.co/h6amECX5K7\\n#News\\n#BBC\\n#CNN\\n#Islam\\n#Truth\\n#god\\n#ISIS\\n#terrorism\\n#Quran\\n#Lies http://t.co/B8iWRdxcm0',\n",
       " 'BLOG: Rain much needed as drought conditions worsen: Right now Charlotte and much of the surrounding area have\\x89Û_ http://t.co/OLzaVTJFKH',\n",
       " '@RetiredFilth people in sydney woke up to the whole sky being red after a dust storm..like unreal.',\n",
       " 'He just wrecked all of you http://t.co/y46isyZkC8',\n",
       " 'In 2014 I will only smoke crqck if I becyme a mayor. This includes Foursquare.',\n",
       " 'You made my mood go from shitty af to panicking af istg',\n",
       " 'I just wanted to watch Paper Towns but the buildings on fire ?????',\n",
       " 'Does This Prepare Us? HHS Selects 9 Regional Special #Pathogen Treatment Centers #Bioterrorism #Infectious #Ebola http://t.co/Qmo1TxxDkj',\n",
       " '#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/cOMuiOk3mP #prebreak #best',\n",
       " 'There is no greater tragedy than becoming comfortable with where you are in life.',\n",
       " '@alexbelloli well now I know lol',\n",
       " \"Attack on Titan game on PS Vita yay! Can't wait for 2016\",\n",
       " 'Emergency crews respond to chemical spill downtown beaumont #benews http://t.co/PME0HOJVYA',\n",
       " 'Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding',\n",
       " 'Am I hearing thunder or trucks?',\n",
       " '#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/MIs0RjxuIr #prebreak #best',\n",
       " 'World Class Tgirl Ass 02 - Scene 4 - Pandemonium http://t.co/iwCu3DgI1a',\n",
       " '#fun #instagramers http://t.co/M3NJvvtYgN\\n\\nJeb Bush said earlier this week that not only does he want to obliterate Planned Parenthood bu\\x89Û_',\n",
       " 'Was in a bad ass mood today got on the elevator at school and decided to make explosion noses everytime some one pressed a button ??']"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting 30 random from text column randomly\n",
    "random.sample(list((train_df.text)),k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data incosistencies or redundant information found in the dataset are as follows\n",
    "\n",
    "* Upper case and lower case at unexpected location\n",
    "* Punctuations\n",
    "* Numbers in text \n",
    "* Use of cities, states and Country names. (Granularity problem)\n",
    "* Special characters such as \\x89ÛÒ and \\n\n",
    "* Hyperklinks\n",
    "* Tags in tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'location' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* Removes texts with numbers\n",
    "* Removes cities names if country/state names are mentioned. (High level granularity is maintained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to chaange text to lower case and remove punctionation\n",
    "def cleaning_location(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    #lower case\n",
    "    text = text.split(',')[-1:][0].strip() # Removing city names when country/state name is present\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  #removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #removing text with number\n",
    "\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Applying the method to location column\n",
    "train_df.location = train_df.location.apply(cleaning_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'keyword' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* replaces number with space (as %20 was found in middle of two words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_keyword(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('(\\d+)', ' ', text)   #replacing numbers with space\n",
    "    return text\n",
    "\n",
    "# Applying the method to keyword column\n",
    "train_df.keyword = train_df.keyword.apply(cleaning_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method extracts the hashtags from the tweets. These hashtags play a vital role in interpreting the context of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_hashtags(text):\n",
    "    hash_tag = ''\n",
    "    hash_tag = re.sub('#','',' '.join(re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', text)))  #retreiving hastags\n",
    "    return hash_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method to clean anomalies in 'text' column does the following\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes words starting with @ to remove the tags and mentions example: @barackobama\n",
    "* Adds a column with hashtag values\n",
    "* Removes links\n",
    "* Removes punctuation\n",
    "* Removes words with numbers\n",
    "* Removes special characters examples: \\x89û,\\x89ûó etc \n",
    "* Removes '\\n' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    \n",
    "    text = re.sub(r'@[A-Za-z]+[A-Za-z0-9-_]+', '',text) #removing any word starting with @   \\w\n",
    "    text = re.sub(r'https|www|http\\S+', '', text)  #removing any word starting with http\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  #removing words with numbers\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # removing special characters\n",
    "    text.replace(\"\\n\",\"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "# Applying the method to retreive hastags and add to the hastag column\n",
    "train_df[\"Hasthags\"] = ''\n",
    "train_df.Hasthags= train_df.text.apply(retreive_hashtags)\n",
    "\n",
    "# Applying the method to the text column in the dataframe\n",
    "train_df.text= train_df.text.apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts NLTK tags to wordnet tags which would be used to lemmatize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    \n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            if tag is None:\n",
    "                #if there is no available tag, append the token as is\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:        \n",
    "                #else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizing.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
