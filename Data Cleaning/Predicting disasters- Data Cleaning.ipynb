{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook goes through a necasssary step of cleaning the data before it is used for exploratory data analysis. \n",
    "* The input of this notebook is a training dataset in csv format sourced from Kaggle. https://www.kaggle.com/c/nlp-getting-started\n",
    "* The output of this notebook is a csv file with clean and lemmatized text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re           # regular expression \n",
    "import string       # String Handling\n",
    "import random       #For selecting random rows\n",
    "import nltk         # Natural langauage processing toolkit\n",
    "from nltk.stem import WordNetLemmatizer  #Used for Lemmatizing the text\n",
    "from nltk.corpus import wordnet          #Used for POS tagging \n",
    "from nltk.corpus import stopwords        #Stopwords to be removed from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training data into a dataframe using pandas and viewing the top 20 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 20 rows in 'Keyword' and 'Location' column are NaNs. Now Checking if there are nulls in other columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of unique values in 'keywords' and 'location'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.keyword.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.location.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 50 rows from the 'locations' column selected randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Australian Capital Territory',\n",
       " 'Wrex',\n",
       " 'Emirates',\n",
       " \"Sharkatraz/Bindle's Cleft, PA\",\n",
       " 'y/e/l',\n",
       " \"satan's colon\",\n",
       " 'Narnia, Maryland',\n",
       " 'Oklahoma',\n",
       " 'ona block w/ my BOY ??',\n",
       " 'Philippines ',\n",
       " 'Winnipeg, Manitoba',\n",
       " 'di langit 7 bidadari (^,^ )',\n",
       " \"Plain O' Texas\",\n",
       " '@notoriousD12',\n",
       " 'Woosley',\n",
       " 'Fresno, CA',\n",
       " \"Jakarta/Kuala Lumpur/S'pore\",\n",
       " 'call me peach or sam lo',\n",
       " 'Leeds, England',\n",
       " 'Burlington, VT',\n",
       " 'Santa Maria, CA',\n",
       " '#HarleyChick#PJNT#RunBenRun',\n",
       " 'England,UK,Europe,Sol 3.',\n",
       " 'Stalybridge, Tameside',\n",
       " 'District of Gentrification/ DC',\n",
       " 'Tripsburg, ms.',\n",
       " 'Lynnfield, MA',\n",
       " 'Dayton, Ohio',\n",
       " 'Eastbourne England',\n",
       " 'Scottsdale, AZ']"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.location.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 50 rows from the 'keywords' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['desolate',\n",
       " 'body%20bag',\n",
       " 'desolation',\n",
       " 'trouble',\n",
       " 'fatal',\n",
       " 'collided',\n",
       " 'wreckage',\n",
       " 'engulfed',\n",
       " 'body%20bagging',\n",
       " 'deluge',\n",
       " 'bioterror',\n",
       " 'blown%20up',\n",
       " 'fear',\n",
       " 'weapon',\n",
       " 'natural%20disaster',\n",
       " 'burned',\n",
       " 'earthquake',\n",
       " 'hellfire',\n",
       " 'heat%20wave',\n",
       " 'windstorm',\n",
       " 'ruin',\n",
       " 'detonate',\n",
       " 'nuclear%20reactor',\n",
       " 'survive',\n",
       " 'dust%20storm',\n",
       " 'riot',\n",
       " 'airplane%20accident',\n",
       " 'tsunami',\n",
       " 'screamed',\n",
       " 'terrorist']"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.keyword.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'text' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Boy 11 charged with manslaughter in shooting death of Elijah Walker http://t.co/HUrIIVFDKC',\n",
       " 'Two air ambulances on scene of serious crash between two cars and lorry in ... - http://t.co/9pFEaQeSki http://t.co/fntG70rnkx | #EMSNe\\x89Û_',\n",
       " 'SANTA CRUZ \\x89ÛÓ Head of the St Elizabeth Police Superintendent Lanford Salmon has r ... - http://t.co/vplR5Hka2u http://t.co/SxHW2TNNLf',\n",
       " \"@OKgooner hahaha great song. 'Spent 15 years getting loaded. 15 years till his liver exploded. Now what's Bob going to do NOW that he...'\",\n",
       " \"Evacuation drill at work. The fire doors wouldn't open so i got to smash the emergency release glass #feelingmanly\",\n",
       " 'Emergency responders prepare for chemical disaster through Hazmat training. http://t.co/q9zixCi8E6',\n",
       " \"The grenade sound effect on 'Impossible' just destroyed about 10 other hoes careers ??\",\n",
       " \"This weekend is me and Nathan's birthday weekend so if you want to drown yourself in beer do reckless things and potentially die hmu\",\n",
       " '@DJJOHNBLazE shout out blaze the hottest DJ in the Sothwest',\n",
       " 'Stemming from my #Cubs talk- the team rosters 2 cancer survivors in @ARizzo44 &amp; @JLester34...@Cubs fans: help another http://t.co/XGnjgLE9eQ',\n",
       " 'BUT I will be uploading these videos ASAP so you guys get to see the new weapon types in action!',\n",
       " 'Homemade frozen yogurt pops? Have you had luck making them? http://t.co/YzaZF4CEOa http://t.co/X5RC5Nuamh',\n",
       " \"#LukeBox something about first responders/ military they are our true Hero's!! Besides your music\",\n",
       " '@deadlydemi even staying up all night to he barrier for tÌüp and then having to run through a dust storm and almost passing out?',\n",
       " '@Hendy_21 sure the purdies will be alive with the blight ??',\n",
       " \"Men escape car engulfed in flames in Parley's Canyon crews investigating cause - http://t.co/YfAVSuOgvl http://t.co/ISI1rLLCt0\",\n",
       " 'Cleveland Heights Shaker Heights fight blight: The House Next Door http://t.co/wYOKt0ftRw',\n",
       " \"My niece just asked me 'would you be scared if there was an apocalypse here?' ????\",\n",
       " \"Woke up to Drake body bagging Meek again!! Meek u can't out spit ya girlfriend... Just lay down Man.... NOT Right... http://t.co/6CraEKc9wb\",\n",
       " '@RaynbowAffair Editor In Chief @DiamondKesawn Releases Issue #7 http://t.co/EbbF1N7MAJ of #RAmag. #Fashion #Models and #Mayhem',\n",
       " '#AyekoRadio play Brasswork Agency - Crushed and Shaken http://t.co/Qh5axvhWH5 #Radio #NetLabel #ElectronicMusic #listen #CCMusic',\n",
       " '@BenignoVito @LibertyBell1000 HILLARYMASS MURDERER.',\n",
       " \"@sarahmcpants @JustJon I'll give him a titty twister\",\n",
       " 'What you gonna do now puppies?! No more destroying my #iPhone Lightning cables! https://t.co/Z4jyHaRreW',\n",
       " \"Please please u gotta listen to @leonalewis # essenceOfMe and thunder it's major\\n#she's #back ????????\",\n",
       " \"#OMG! I don't believe this. #RIP bro\\n#AirPlane #Accident #JetEngine #TurboJet #Boing #G90 http://t.co/KXxnSZp6nk\",\n",
       " 'The Latest: More homes razed by Northern California wildfire: The latest on wildfires burning in California and\\x89Û_ http://t.co/0Keh2TReNy',\n",
       " 'AH-Mazing story of the power animal rescuers have! A starving homeless dog with no future was rescued by a person... http://t.co/ficd5qbqwl',\n",
       " 'Rory McIlroy to Test Ankle Injury in Weekend Practice #chinadotcom #sports http://t.co/UDTGWfSc3P http://t.co/V5wSx0LQN2',\n",
       " 'When you go to a concert and someone screams in your ear... Does it look like I wanna loose my hearing anytime soon???']"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting 30 random from text column randomly\n",
    "random.sample(list((train_df.text)),k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data incosistencies or redundant information found in the dataset are as follows\n",
    "\n",
    "* Upper case and lower case at unexpected location\n",
    "* Punctuations\n",
    "* Numbers in text \n",
    "* Use of cities, states and Country names. (Granularity problem)\n",
    "* Special characters such as \\x89ÛÒ and \\n\n",
    "* Hyperklinks\n",
    "* Tags in tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'location' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* Removes texts with numbers\n",
    "* Removes cities names if country/state names are mentioned. (High level granularity is maintained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to chaange text to lower case and remove punctionation\n",
    "def cleaning_location(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    #lower case\n",
    "    text = text.split(',')[-1:][0].strip() # Removing city names when country/state name is present\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  #removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #removing text with number\n",
    "    return text\n",
    "\n",
    "# Applying the method to location column\n",
    "train_df.location = train_df.location.apply(cleaning_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'keyword' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* replaces number with space (as %20 was found in middle of two words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_keyword(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('(\\d+)', ' ', text)   #replacing numbers with space\n",
    "    return text\n",
    "\n",
    "# Applying the method to keyword column\n",
    "train_df.keyword = train_df.keyword.apply(cleaning_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method extracts the hashtags from the tweets. These hashtags play a vital role in interpreting the context of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_hashtags(text):\n",
    "    hash_tag = ''\n",
    "    hash_tag = re.sub('#','',' '.join(re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', text)))  #retreiving hastags\n",
    "    return hash_tag\n",
    "\n",
    "# Applying the method to retreive hastags and add to the hastag column\n",
    "train_df[\"Hasthags\"] = ''\n",
    "train_df.Hasthags= train_df.text.apply(retreive_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method to clean anomalies in 'text' column does the following\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes words starting with @ to remove the tags and mentions example: @barackobama\n",
    "* Adds a column with hashtag values\n",
    "* Removes links\n",
    "* Removes punctuation\n",
    "* Removes words with numbers\n",
    "* Removes special characters examples: \\x89û,\\x89ûó etc \n",
    "* Removes '\\n' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    \n",
    "    text = re.sub(r'@[A-Za-z]+[A-Za-z0-9-_]+', '',text) #removing any word starting with @   \\w\n",
    "    text = re.sub(r'https|www|http\\S+', '', text)  #removing any word starting with http\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  #removing words with numbers\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # removing special characters\n",
    "    text.replace(\"\\n\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts NLTK tags to wordnet tags which would be used to lemmatize the words in the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method is used to lemmatize the sentences in the following order:\n",
    "\n",
    "* Clean the sentences by calling the cleaning_text method on each sentence.\n",
    "* Tokenizing the sentence and generating a nltk POS tag for each word in the cleaned sentence.\n",
    "* Converting the nltk POS tag to Wordnet POS tag by calling wordnet_tag method.\n",
    "* Removes the stopwords\n",
    "* Lemmatizes the tokens using the pos tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence = cleaning_text(sentence) #Cleaning the sentence\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  #Tokenizing and tagging each word \n",
    "    wordnet_tagged = map(lambda x: (x[0], wordnet_tag(x[1])), nltk_tagged)  # Coverting the NLTK tags to wordnet tag\n",
    "    \n",
    "    #Lemmatizing the tagged tokens\n",
    "    lemmatized_sentence = [] #empty list for lemmatized words\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if word not in set(stopwords.words('english')):  #removing stopwords\n",
    "            if tag is None:                   \n",
    "                lemmatized_sentence.append(word) #adding the word as it is if POS tag missing\n",
    "            else:        \n",
    "                #else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizing.lemmatize(word, tag))  ##lemmatizing the token using the POS tag\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the lemmatizing method to Hashtags column \n",
    "train_df.Hasthags= train_df.Hasthags.apply(lemmatize_sentence)\n",
    "\n",
    "# Applying the lemmatizing method to text column \n",
    "train_df.text= train_df.text.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Hasthags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2614</td>\n",
       "      <td>3752</td>\n",
       "      <td>destruction</td>\n",
       "      <td>nan</td>\n",
       "      <td>crackdown destruction restrict multiplayer cra...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1744</td>\n",
       "      <td>buildings burning</td>\n",
       "      <td>il</td>\n",
       "      <td>infosec rather know firefighter day often run ...</td>\n",
       "      <td>0</td>\n",
       "      <td>infosec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6322</td>\n",
       "      <td>9038</td>\n",
       "      <td>stretcher</td>\n",
       "      <td>florida</td>\n",
       "      <td>restore vinyl siding make look new</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2918</td>\n",
       "      <td>4197</td>\n",
       "      <td>drown</td>\n",
       "      <td>inside your webcam stop that</td>\n",
       "      <td>send drown like way u think yes</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>183</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>marysville ca</td>\n",
       "      <td>praise god ministry tell like wdyouth biblestudy</td>\n",
       "      <td>0</td>\n",
       "      <td>wdyouth biblestudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2603</td>\n",
       "      <td>3737</td>\n",
       "      <td>destroyed</td>\n",
       "      <td>nan</td>\n",
       "      <td>hi david saw usa walk war destroy life million...</td>\n",
       "      <td>1</td>\n",
       "      <td>irandeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>1500</td>\n",
       "      <td>body bags</td>\n",
       "      <td>tx</td>\n",
       "      <td>bring body bag tho</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5677</td>\n",
       "      <td>8101</td>\n",
       "      <td>rescued</td>\n",
       "      <td>west hollywood</td>\n",
       "      <td>summer summervibes california puppy pitmix res...</td>\n",
       "      <td>0</td>\n",
       "      <td>summervibes california puppy pitmix rescue bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5492</td>\n",
       "      <td>7837</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>nan</td>\n",
       "      <td>reddit quarantine offensivecontent</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4066</td>\n",
       "      <td>5778</td>\n",
       "      <td>forest fires</td>\n",
       "      <td>bc</td>\n",
       "      <td>forest fire could delay official say could goo...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>1078</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>valencia</td>\n",
       "      <td>say bad thing happen reason wise word gon na s...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>1245</td>\n",
       "      <td>blood</td>\n",
       "      <td>nan</td>\n",
       "      <td>cant believe people mid dont high blood pressu...</td>\n",
       "      <td>1</td>\n",
       "      <td>decisionsondecisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4152</td>\n",
       "      <td>5901</td>\n",
       "      <td>harm</td>\n",
       "      <td>nan</td>\n",
       "      <td>inner man wayward route harm spent replace onl...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>4132</td>\n",
       "      <td>drought</td>\n",
       "      <td>texas</td>\n",
       "      <td>californian like living africas sahel desert f...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7463</td>\n",
       "      <td>10679</td>\n",
       "      <td>wounds</td>\n",
       "      <td>not steven yeun  amc</td>\n",
       "      <td>man gently dab cotton rag across one wound pai...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5977</td>\n",
       "      <td>8536</td>\n",
       "      <td>screaming</td>\n",
       "      <td>nan</td>\n",
       "      <td>im scream</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5535</td>\n",
       "      <td>7895</td>\n",
       "      <td>radiation emergency</td>\n",
       "      <td>mn usa</td>\n",
       "      <td>nuclear emergency current usa radiation level ...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5428</td>\n",
       "      <td>7747</td>\n",
       "      <td>police</td>\n",
       "      <td>mo</td>\n",
       "      <td>police gunman report dead nashville area theat...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4295</td>\n",
       "      <td>6100</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>colorado</td>\n",
       "      <td>say rude base experience kind next level sub t...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2786</td>\n",
       "      <td>4008</td>\n",
       "      <td>disaster</td>\n",
       "      <td>chillin at ceder rapids</td>\n",
       "      <td>beautiful disaster jon mclaughlin good song</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6891</td>\n",
       "      <td>9878</td>\n",
       "      <td>traumatised</td>\n",
       "      <td>lowestoft</td>\n",
       "      <td>funny im traumatise</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6039</td>\n",
       "      <td>8633</td>\n",
       "      <td>seismic</td>\n",
       "      <td>somalia</td>\n",
       "      <td>exploration take seismic shift gabon somalia</td>\n",
       "      <td>1</td>\n",
       "      <td>gabon somalia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>249</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>nan</td>\n",
       "      <td>twelve fear kill pakistani air ambulance helic...</td>\n",
       "      <td>1</td>\n",
       "      <td>yugvani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5684</td>\n",
       "      <td>8111</td>\n",
       "      <td>rescued</td>\n",
       "      <td>nan</td>\n",
       "      <td>fund need rescue abandon cocker spaniel</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6877</td>\n",
       "      <td>9860</td>\n",
       "      <td>traumatised</td>\n",
       "      <td>kirkwall</td>\n",
       "      <td>deep road tho hahahaha im traumatise deep road...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>2265</td>\n",
       "      <td>cliff fall</td>\n",
       "      <td>nan</td>\n",
       "      <td>youre reading go accidentally fall cliff mate</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5057</td>\n",
       "      <td>7206</td>\n",
       "      <td>natural disaster</td>\n",
       "      <td>nan</td>\n",
       "      <td>ral send message condolence vietnam follow nat...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2221</td>\n",
       "      <td>3179</td>\n",
       "      <td>deluge</td>\n",
       "      <td>nz</td>\n",
       "      <td>light rain forecast base dress light rain ince...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4476</td>\n",
       "      <td>6366</td>\n",
       "      <td>hostages</td>\n",
       "      <td>china</td>\n",
       "      <td>hot specially modify land stadium rescue hosta...</td>\n",
       "      <td>0</td>\n",
       "      <td>hot prebreak best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7298</td>\n",
       "      <td>10444</td>\n",
       "      <td>wild fires</td>\n",
       "      <td>ca</td>\n",
       "      <td>map show california wildfire burn map create c...</td>\n",
       "      <td>1</td>\n",
       "      <td>fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword                      location  \\\n",
       "2614   3752          destruction                           nan   \n",
       "1210   1744    buildings burning                            il   \n",
       "6322   9038            stretcher                       florida   \n",
       "2918   4197                drown  inside your webcam stop that   \n",
       "127     183           aftershock                 marysville ca   \n",
       "2603   3737            destroyed                           nan   \n",
       "1034   1500            body bags                            tx   \n",
       "5677   8101              rescued                west hollywood   \n",
       "5492   7837           quarantine                           nan   \n",
       "4066   5778         forest fires                            bc   \n",
       "746    1078             bleeding                      valencia   \n",
       "863    1245                blood                           nan   \n",
       "4152   5901                 harm                           nan   \n",
       "2875   4132              drought                         texas   \n",
       "7463  10679               wounds          not steven yeun  amc   \n",
       "5977   8536            screaming                           nan   \n",
       "5535   7895  radiation emergency                        mn usa   \n",
       "5428   7747               police                            mo   \n",
       "4295   6100             hellfire                      colorado   \n",
       "2786   4008             disaster       chillin at ceder rapids   \n",
       "6891   9878          traumatised                     lowestoft   \n",
       "6039   8633              seismic                       somalia   \n",
       "174     249            ambulance                           nan   \n",
       "5684   8111              rescued                           nan   \n",
       "6877   9860          traumatised                      kirkwall   \n",
       "1568   2265           cliff fall                           nan   \n",
       "5057   7206     natural disaster                           nan   \n",
       "2221   3179               deluge                            nz   \n",
       "4476   6366             hostages                         china   \n",
       "7298  10444           wild fires                            ca   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2614  crackdown destruction restrict multiplayer cra...       0   \n",
       "1210  infosec rather know firefighter day often run ...       0   \n",
       "6322                 restore vinyl siding make look new       0   \n",
       "2918                    send drown like way u think yes       0   \n",
       "127    praise god ministry tell like wdyouth biblestudy       0   \n",
       "2603  hi david saw usa walk war destroy life million...       1   \n",
       "1034                                 bring body bag tho       0   \n",
       "5677  summer summervibes california puppy pitmix res...       0   \n",
       "5492                 reddit quarantine offensivecontent       0   \n",
       "4066  forest fire could delay official say could goo...       1   \n",
       "746   say bad thing happen reason wise word gon na s...       0   \n",
       "863   cant believe people mid dont high blood pressu...       1   \n",
       "4152  inner man wayward route harm spent replace onl...       0   \n",
       "2875  californian like living africas sahel desert f...       1   \n",
       "7463  man gently dab cotton rag across one wound pai...       0   \n",
       "5977                                          im scream       0   \n",
       "5535  nuclear emergency current usa radiation level ...       0   \n",
       "5428  police gunman report dead nashville area theat...       1   \n",
       "4295  say rude base experience kind next level sub t...       0   \n",
       "2786        beautiful disaster jon mclaughlin good song       0   \n",
       "6891                                funny im traumatise       0   \n",
       "6039       exploration take seismic shift gabon somalia       1   \n",
       "174   twelve fear kill pakistani air ambulance helic...       1   \n",
       "5684            fund need rescue abandon cocker spaniel       0   \n",
       "6877  deep road tho hahahaha im traumatise deep road...       0   \n",
       "1568      youre reading go accidentally fall cliff mate       0   \n",
       "5057  ral send message condolence vietnam follow nat...       1   \n",
       "2221  light rain forecast base dress light rain ince...       0   \n",
       "4476  hot specially modify land stadium rescue hosta...       0   \n",
       "7298  map show california wildfire burn map create c...       1   \n",
       "\n",
       "                                               Hasthags  \n",
       "2614                                                     \n",
       "1210                                            infosec  \n",
       "6322                                                     \n",
       "2918                                                yes  \n",
       "127                                  wdyouth biblestudy  \n",
       "2603                                           irandeal  \n",
       "1034                                                     \n",
       "5677  summervibes california puppy pitmix rescue bri...  \n",
       "5492                                                     \n",
       "4066                                                     \n",
       "746                                                      \n",
       "863                                decisionsondecisions  \n",
       "4152                                                     \n",
       "2875                                                     \n",
       "7463                                                     \n",
       "5977                                                     \n",
       "5535                                                     \n",
       "5428                                                     \n",
       "4295                                                     \n",
       "2786                                                     \n",
       "6891                                                     \n",
       "6039                                      gabon somalia  \n",
       "174                                             yugvani  \n",
       "5684                                                     \n",
       "6877                                                     \n",
       "1568                                                     \n",
       "5057                                                     \n",
       "2221                                                     \n",
       "4476                                  hot prebreak best  \n",
       "7298                                               fire  "
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the cleaned dataframe as csv to be used for Modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"Clean_train_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
