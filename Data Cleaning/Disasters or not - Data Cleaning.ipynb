{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook goes through a necasssary step of cleaning the data before it is used for exploratory data analysis. \n",
    "* The input of this notebook is a training dataset in csv format sourced from Kaggle. https://www.kaggle.com/c/nlp-getting-started\n",
    "* The output of this notebook is a csv file with clean and lemmatized text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re           # regular expression \n",
    "import string       # String Handling\n",
    "import random       #For selecting random rows\n",
    "import nltk         # Natural langauage processing toolkit\n",
    "from nltk.stem import WordNetLemmatizer  #Used for Lemmatizing the text\n",
    "from nltk.corpus import wordnet          #Used for POS tagging \n",
    "from nltk.corpus import stopwords        #Stopwords to be removed from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training data into a dataframe using pandas and viewing the top 20 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 20 rows in 'Keyword' and 'Location' column are NaNs. Now Checking if there are nulls in other columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of unique values in 'keywords' and 'location'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.keyword.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.location.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'locations' column selected randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Host of #MindMoversPodcast',\n",
       " 'United States of America',\n",
       " 'New Jersey, usually',\n",
       " 'Phoenix',\n",
       " 'Victoria, Australia, Earth',\n",
       " '#expelcl*y',\n",
       " 'Vancouver, Canada',\n",
       " ' New England',\n",
       " '3?3?7?SLOPelousas??2?2?5?',\n",
       " 'CAMARILLO, CA',\n",
       " 'Benton City, Washington',\n",
       " \"wrapped arnd hyuk's finger\",\n",
       " 'Oklahoma',\n",
       " 'Gurgaon, Haryana. ',\n",
       " 'SEATTLE, WA USA',\n",
       " 'lee london',\n",
       " 'y/e/l',\n",
       " 'Savannah, GA',\n",
       " 'denver colorado',\n",
       " 'Greenville',\n",
       " 'amsterdayum 120615 062415',\n",
       " 'Happily Married with 2 kids ',\n",
       " 'Reading MA',\n",
       " 'Australian Capital Territory',\n",
       " 'Ellensburg to Spokane',\n",
       " ' ?currently writing a book?',\n",
       " 'Chicago, IL',\n",
       " 'Danville, VA',\n",
       " 'Purgatory, USA',\n",
       " 'Griffin :3']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.location.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'keywords' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flames',\n",
       " 'wreck',\n",
       " 'blood',\n",
       " 'casualty',\n",
       " 'trapped',\n",
       " 'devastation',\n",
       " 'electrocute',\n",
       " 'detonation',\n",
       " 'outbreak',\n",
       " 'emergency%20plan',\n",
       " 'demolished',\n",
       " 'detonate',\n",
       " 'injury',\n",
       " 'annihilated',\n",
       " 'sinking',\n",
       " 'disaster',\n",
       " 'injuries',\n",
       " 'drowned',\n",
       " 'blew%20up',\n",
       " 'sunk',\n",
       " 'wreckage',\n",
       " 'fire',\n",
       " 'ruin',\n",
       " 'rescued',\n",
       " 'hazard',\n",
       " 'wildfire',\n",
       " 'rubble',\n",
       " 'floods',\n",
       " 'blown%20up',\n",
       " 'aftershock']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.keyword.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'text' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@TemecaFreeman GM! I pray any attack of the enemy 2 derail ur destiny is blocked by the Lord &amp; that He floods ur life w/heavenly Blessings',\n",
       " 'First Time Playing Hearthstone on PC Thoughts http://t.co/aBoLxMH1vy',\n",
       " \"'We are now prepared to obliterate more rapidly and completely every productive enterprise the Japanese have above ground in any city.'\",\n",
       " 'i be on that hotboy shit',\n",
       " '@freefromwolves GodsLove &amp; #thankU brother Danny for RT of NEW VIDEO http://t.co/cybKsXHF7d The Coming Apocalyptic US Earthquake &amp; Tsunami',\n",
       " \"#Greece's tax revenues collapse as debt crisis continues via @guardian #bailout http://t.co/cJvbQXw83s ^mp\",\n",
       " 'Black Eye 9: A space battle occurred at Star O784 involving 2 fleets totaling 3939 ships with 11 destroyed',\n",
       " \"We're #hiring! Read about our latest #job opening here: RN Nurse Shift Manager Emergency Services - Full time... - http://t.co/sNuBZA6KSC\",\n",
       " 'My asshole is on fire  https://t.co/Y3FO0gHg8t',\n",
       " '#hot  C-130 specially modified to land in a stadium and rescue hostages in Iran in 1980 http://t.co/OnvD9D4NKg #prebreak #best',\n",
       " '#hot  Funtenna: hijacking computers to send data as sound waves [Black Hat 2015] http://t.co/s4PNIhJQX7 #prebreak #best',\n",
       " '@chxrmingprince @jones_luna I should bloody hope so *she said folding her arms sitting back in her chair*',\n",
       " 'It was finally demolished in the spring of 2013 and the property has sat vacant since. The just\\x89Û_: saddlebrooke... http://t.co/Vcjcykq8b8',\n",
       " 'Maxsys is hiring a #Demolition #Workers apply now! #Halifax #jobs http://t.co/QTIZcBWw7G',\n",
       " 'Back in Ireland v. sad/traumatised as is freezing and not beautiful Parisian summer to which have become accustomed.',\n",
       " 'BREAKING: Obama Officials GAVE Muslim Terrorist the Weapon Used in Texas Attack http://t.co/RJcaxjp4oS',\n",
       " 'UPDATE: 7 of the 9 Mac Pros my company bought in May have had catastrophe failures requiring repair!',\n",
       " 'DLH issues Hazardous Weather Outlook (HWO) http://t.co/WOzuBXRi2p',\n",
       " '@freddiedeboer @Thucydiplease then you have rise of Coates Charleston massacre Walter Scott and black twitter more broadly as well.',\n",
       " '@POTUS Thx for emergency dec. http://t.co/DyWWNbbYvJ 4 days and no plan to get H20 to those who have no transport. Can you deploy troops?',\n",
       " 'The Bush fires in CA are so crazy',\n",
       " 'Drunk twister is so hard ????',\n",
       " 'OH MY GOD RYANS IN TROUBLE http://t.co/ADIp0UnXHU',\n",
       " 'School Of Seven Bells - Windstorm  http://t.co/E1kbluDwh5 #nowplaying',\n",
       " 'New post from @SeismicSoftware: 3 Major Challenges of Channel Sales Enablement http://t.co/kWMRCEkVTF',\n",
       " \"CAMILA'S DOING A FOLLOW SPREE TONIGHT IM SCREAMING OF HAPPINESS\",\n",
       " 'One Year on from the Sinjar Massacre #Yazidis Blast Lack of Action Over Hostages\\nhttp://t.co/BAqOcMcJqc',\n",
       " \"I'm an emotional wreck right now.\",\n",
       " \"Winter Desolation of Death is also on Tumblr:  http://t.co/93DM6gnWwC  Al Necro's reviews interviews &amp; more!\",\n",
       " 'Calgary Police Flood Road Closures in Calgary. http://t.co/RLN09WKe9g']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting 30 random from text column randomly\n",
    "random.sample(list((train_df.text)),k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data incosistencies or redundant information found in the dataset are as follows\n",
    "\n",
    "* Upper case and lower case at unexpected location\n",
    "* Punctuations\n",
    "* Numbers in text \n",
    "* Use of cities, states and Country names. (Granularity problem)\n",
    "* Special characters such as \\x89ÛÒ and \\n\n",
    "* Hyperklinks\n",
    "* Tags in tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'location' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* Removes texts with numbers\n",
    "* Removes cities names if country/state names are mentioned. (High level granularity is maintained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to chaange text to lower case and remove punctionation\n",
    "def cleaning_location(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    #lower case\n",
    "    text = text.split(',')[-1:][0].strip() # Removing city names when country/state name is present\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  #removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #removing text with number\n",
    "    return text\n",
    "\n",
    "# Applying the method to location column\n",
    "train_df.location = train_df.location.apply(cleaning_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'keyword' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* replaces number with space (as %20 was found in middle of two words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_keyword(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('(\\d+)', ' ', text)   #replacing numbers with space\n",
    "    return text\n",
    "\n",
    "# Applying the method to keyword column\n",
    "train_df.keyword = train_df.keyword.apply(cleaning_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method to clean anomalies in 'text' column does the following\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes words starting with @ to remove the tags and mentions example: @barackobama\n",
    "* Adds a column with hashtag values\n",
    "* Removes links\n",
    "* Removes punctuation\n",
    "* Removes words with numbers\n",
    "* Removes special characters examples: \\x89û,\\x89ûó etc \n",
    "* Removes '\\n' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    \n",
    "    text = re.sub(r'@[A-Za-z]+[A-Za-z0-9-_]+', '',text) #removing any word starting with @   \\w\n",
    "    text = re.sub(r'https|www|http\\S+', '', text)  #removing any word starting with http\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  #removing words with numbers\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # removing special characters\n",
    "    text.replace(\"\\n\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts NLTK tags to wordnet tags which would be used to lemmatize the words in the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method is used to lemmatize the sentences in the following order:\n",
    "\n",
    "* Clean the sentences by calling the cleaning_text method on each sentence.\n",
    "* Tokenizing the sentence and generating a nltk POS tag for each word in the cleaned sentence.\n",
    "* Converting the nltk POS tag to Wordnet POS tag by calling wordnet_tag method.\n",
    "* Removes the stopwords\n",
    "* Lemmatizes the tokens using the pos tags and joining them to form a sentence of lemmatized words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence = cleaning_text(sentence) #Cleaning the sentence\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  #Tokenizing and tagging each word \n",
    "    wordnet_tagged = map(lambda x: (x[0], wordnet_tag(x[1])), nltk_tagged)  # Coverting the NLTK tags to wordnet tag\n",
    "    \n",
    "    #Lemmatizing the tagged tokens\n",
    "    lemmatized_sentence = [] #empty list for lemmatized words\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if word not in set(stopwords.words('english')):  #removing stopwords\n",
    "            if tag is None:                   \n",
    "                lemmatized_sentence.append(word) #adding the word as it is if POS tag missing\n",
    "            else:        \n",
    "                #else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizing.lemmatize(word, tag))  ##lemmatizing the token using the POS tag\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the lemmatizing method to text column \n",
    "train_df.text= train_df.text.apply(lemmatize_sentence)\n",
    "\n",
    "#Applying the lemmarizing methiod to keyword column\n",
    "train_df.keyword = train_df.keyword.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>4680</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>nan</td>\n",
       "      <td>feel engulfed low selfimage take quiz</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5327</td>\n",
       "      <td>7605</td>\n",
       "      <td>pandemonium</td>\n",
       "      <td>india</td>\n",
       "      <td>govt pass bill pandemonium upa use cant nda</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>7493</td>\n",
       "      <td>obliteration</td>\n",
       "      <td>countries and counting</td>\n",
       "      <td>fear mind killer fear littledeath bring total ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5706</td>\n",
       "      <td>8142</td>\n",
       "      <td>rescuer</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>video pick body water rescuer search hundred m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>1058</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>il</td>\n",
       "      <td>fell someone back hit head concrete bleed n shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4448</td>\n",
       "      <td>6330</td>\n",
       "      <td>hostage</td>\n",
       "      <td>ca</td>\n",
       "      <td>wut lonely lunch get ditch im hungry hostage</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5491</td>\n",
       "      <td>7835</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>nan</td>\n",
       "      <td>reddit quarantine offensive content</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1538</td>\n",
       "      <td>2221</td>\n",
       "      <td>chemical emergency</td>\n",
       "      <td>tx</td>\n",
       "      <td>emergency crew respond chemical spill downtown...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1861</td>\n",
       "      <td>2675</td>\n",
       "      <td>crush</td>\n",
       "      <td>nan</td>\n",
       "      <td>ron amp fez daves high school crush via</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5849</td>\n",
       "      <td>8360</td>\n",
       "      <td>ruin</td>\n",
       "      <td>followurdreams my instagram</td>\n",
       "      <td>fresh da shower look still love new hair ruin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5479</td>\n",
       "      <td>7818</td>\n",
       "      <td>quarantine</td>\n",
       "      <td>co</td>\n",
       "      <td>reddit quarantine offensive content reddit cof...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1211</td>\n",
       "      <td>1745</td>\n",
       "      <td>building burn</td>\n",
       "      <td>i think</td>\n",
       "      <td>burn fucking building</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6503</td>\n",
       "      <td>9297</td>\n",
       "      <td>survive</td>\n",
       "      <td>united states</td>\n",
       "      <td>ahh forget headphone suppose survive day witho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2954</td>\n",
       "      <td>4245</td>\n",
       "      <td>drown</td>\n",
       "      <td>tampa</td>\n",
       "      <td>felt remorse miss pam drown guy really like ja...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2224</td>\n",
       "      <td>3182</td>\n",
       "      <td>deluge</td>\n",
       "      <td>usa</td>\n",
       "      <td>audio business owner share would differently</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4302</td>\n",
       "      <td>6109</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>colorado</td>\n",
       "      <td>message send dont reply see saw message least ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6434</td>\n",
       "      <td>9207</td>\n",
       "      <td>suicide bombing</td>\n",
       "      <td>nan</td>\n",
       "      <td>suicide bomb location name premonition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>1480</td>\n",
       "      <td>body bag</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>micom summer contrast candy color bowknot cros...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3821</td>\n",
       "      <td>5432</td>\n",
       "      <td>first responder</td>\n",
       "      <td>nan</td>\n",
       "      <td>juneau empire first responder turn national night</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3063</td>\n",
       "      <td>4395</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>wa</td>\n",
       "      <td>sure megaquake story bring sense panic questio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword                     location  \\\n",
       "3258  4680            engulfed                          nan   \n",
       "5327  7605         pandemonium                        india   \n",
       "5240  7493        obliteration       countries and counting   \n",
       "5706  8142             rescuer                      nigeria   \n",
       "732   1058            bleeding                           il   \n",
       "4448  6330             hostage                           ca   \n",
       "5491  7835          quarantine                          nan   \n",
       "1538  2221  chemical emergency                           tx   \n",
       "1861  2675               crush                          nan   \n",
       "5849  8360                ruin  followurdreams my instagram   \n",
       "5479  7818          quarantine                           co   \n",
       "1211  1745       building burn                      i think   \n",
       "6503  9297             survive                united states   \n",
       "2954  4245               drown                        tampa   \n",
       "2224  3182              deluge                          usa   \n",
       "4302  6109            hellfire                     colorado   \n",
       "6434  9207     suicide bombing                          nan   \n",
       "1019  1480            body bag                oklahoma city   \n",
       "3821  5432     first responder                          nan   \n",
       "3063  4395          earthquake                           wa   \n",
       "\n",
       "                                                   text  target  \n",
       "3258              feel engulfed low selfimage take quiz       0  \n",
       "5327        govt pass bill pandemonium upa use cant nda       0  \n",
       "5240  fear mind killer fear littledeath bring total ...       0  \n",
       "5706  video pick body water rescuer search hundred m...       1  \n",
       "732    fell someone back hit head concrete bleed n shit       0  \n",
       "4448       wut lonely lunch get ditch im hungry hostage       0  \n",
       "5491                reddit quarantine offensive content       0  \n",
       "1538  emergency crew respond chemical spill downtown...       1  \n",
       "1861            ron amp fez daves high school crush via       1  \n",
       "5849  fresh da shower look still love new hair ruin ...       0  \n",
       "5479  reddit quarantine offensive content reddit cof...       0  \n",
       "1211                              burn fucking building       1  \n",
       "6503  ahh forget headphone suppose survive day witho...       0  \n",
       "2954  felt remorse miss pam drown guy really like ja...       0  \n",
       "2224       audio business owner share would differently       0  \n",
       "4302  message send dont reply see saw message least ...       0  \n",
       "6434             suicide bomb location name premonition       1  \n",
       "1019  micom summer contrast candy color bowknot cros...       0  \n",
       "3821  juneau empire first responder turn national night       0  \n",
       "3063  sure megaquake story bring sense panic questio...       1  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if text from any row was completely removed due to cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5115</td>\n",
       "      <td>7295</td>\n",
       "      <td>nuclear reactor</td>\n",
       "      <td>nan</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          keyword location text  target\n",
       "5115  7295  nuclear reactor      nan            0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df.text == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one row with id 7295 where the whole text was removed. \n",
    "\n",
    "The original train data is read again to see the content of the row with id 7295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5115</td>\n",
       "      <td>7295</td>\n",
       "      <td>nuclear%20reactor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Err:509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            keyword location     text  target\n",
       "5115  7295  nuclear%20reactor      NaN  Err:509       0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crosscheck = pd.read_csv('train.csv')\n",
    "df_crosscheck.loc[df_crosscheck.id == 7295]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed above that the text in that particular row is an error message. Hence, that text was correctly cleaned from that row. \n",
    "\n",
    "The row is removed from the dataframe and the dataframe is saved as a csv file to be used for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7612 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7612 non-null int64\n",
      "keyword     7612 non-null object\n",
      "location    7612 non-null object\n",
      "text        7612 non-null object\n",
      "target      7612 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 356.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df.text != '']\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"Clean_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning test data using the predefined methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the method to location column\n",
    "test_df.location = test_df.location.apply(cleaning_location)\n",
    "# Applying the method to keyword column\n",
    "test_df.keyword = test_df.keyword.apply(cleaning_keyword)\n",
    "\n",
    "# Applying the lemmatizing method to text column \n",
    "test_df.text= test_df.text.apply(lemmatize_sentence)\n",
    "\n",
    "# Applying the lemmatizing method to keyword column \n",
    "test_df.keyword= test_df.keyword.apply(lemmatize_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving test dataset as csv\n",
    "test_df.to_csv('Clean_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
