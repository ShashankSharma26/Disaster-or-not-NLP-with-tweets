{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook goes through a necasssary step of cleaning the data before it is used for exploratory data analysis. \n",
    "* The input of this notebook is a training dataset in csv format sourced from Kaggle. https://www.kaggle.com/c/nlp-getting-started\n",
    "* The output of this notebook is a csv file with clean and lemmatized text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Understanding the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re           # regular expression \n",
    "import string       # String Handling\n",
    "import random       #For selecting random rows\n",
    "import nltk         # Natural langauage processing toolkit\n",
    "from nltk.stem import WordNetLemmatizer  #Used for Lemmatizing the text\n",
    "from nltk.corpus import wordnet          #Used for POS tagging \n",
    "from nltk.corpus import stopwords        #Stopwords to be removed from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the training data into a dataframe using pandas and viewing the top 20 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 20 rows in 'Keyword' and 'Location' column are NaNs. Now Checking if there are nulls in other columns of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7613 non-null int64\n",
      "keyword     7552 non-null object\n",
      "location    5080 non-null object\n",
      "text        7613 non-null object\n",
      "target      7613 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of unique values in 'keywords' and 'location'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.keyword.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3342"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df.location.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'locations' column selected randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im definitely taller than you.',\n",
       " 'Made Here In Detroit ',\n",
       " 'Haddonfield, NJ',\n",
       " 'Pig Symbol, Alabama',\n",
       " 'at my home',\n",
       " 'North West London',\n",
       " 'EGYPT',\n",
       " 'Georgia, U.S.A.',\n",
       " '??????',\n",
       " 'Ontario, Canada. ',\n",
       " 'Minneapolis - St. Paul',\n",
       " 'THE WORLD T.G.G / M.M.M ',\n",
       " 'Utica NY',\n",
       " 'Aztec NM',\n",
       " 'Reading a romance novel',\n",
       " 'IRAQ',\n",
       " 'MontrÌ©al, QuÌ©bec',\n",
       " '36 & 38',\n",
       " 'Los Angeles',\n",
       " 'iamdigitalent.com',\n",
       " '?? ??',\n",
       " 'Cymru araul',\n",
       " 'playing soccer & eating pizza',\n",
       " 'planeta H2o',\n",
       " 'Albuquerque',\n",
       " 'Warm Heart Of Africa',\n",
       " 'Sydney, New South Wales',\n",
       " 'we?it \\x89Û¢ ixwin',\n",
       " 'U.S.A',\n",
       " 'Wakanda']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.location.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'keywords' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural%20disaster',\n",
       " 'sunk',\n",
       " 'volcano',\n",
       " 'death',\n",
       " 'destruction',\n",
       " 'blaze',\n",
       " 'burning',\n",
       " 'bombing',\n",
       " 'earthquake',\n",
       " 'twister',\n",
       " 'terrorism',\n",
       " 'crashed',\n",
       " 'annihilation',\n",
       " 'rainstorm',\n",
       " 'smoke',\n",
       " 'collided',\n",
       " 'outbreak',\n",
       " 'screams',\n",
       " 'body%20bags',\n",
       " 'storm',\n",
       " 'desolate',\n",
       " 'thunderstorm',\n",
       " 'drown',\n",
       " 'detonate',\n",
       " 'panic',\n",
       " 'hazard',\n",
       " 'massacre',\n",
       " 'disaster',\n",
       " 'demolish',\n",
       " 'floods']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list((train_df.keyword.unique())),k=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at 30 rows from the 'text' column selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Latest: Washington #Wildfire misses town; evacuations end - KHQ Right Now http://t.co/aNlhW2IzkZ',\n",
       " 'Reality Training: Train falls off elevated tracks during windstorm http://t.co/JIOMnrCygT #Paramedic #EMS',\n",
       " 'Watch This Airport Get Swallowed Up By A Sandstorm In Under A Minute http://t.co/VZPKn23RX4',\n",
       " \"@pmharper don't worry I'm sure climate has nothing to do with hail floods tornados in Alberta &amp; Ontario. I'm sure god's just mad at you.\",\n",
       " \"Madhya Pradesh Train Derailment: Village Youth Saved Many Lives: A group of villagers saved over 70 passengers' lives after two train...\",\n",
       " 'Imagini noi si 2 clipuri The Hobbit: The Desolation of Smaug -... http://t.co/j6CfwUKofE #cliptv #desolationofsmaug #poze #thehobbit',\n",
       " 'Beautiful lightning as seen from plane window http://t.co/5CwUyLnFUm http://t.co/1tyYqFz13D',\n",
       " 'Watch This Airport Get Swallowed Up By A Sandstorm In Under A Minute http://t.co/sEquWmvFx4',\n",
       " 'Battle of the GOATS  https://t.co/ofECs6tcvC',\n",
       " 'Global precipitation measurement satellite captures 3-D image of Typhoon Soudelor - NASAHurricane. Visit: http://t.co/sQN4girdvZ',\n",
       " 'Virgin galactic crash: early unlocking of brakes triggered structural failure - Irish Examiner http://t.co/ocMCvfDZkv',\n",
       " 'Not only are you a mass murderer but at a movie theatre where niggas dropped bread to see a movie? Cmon man.',\n",
       " '#Autoinsurance industry clueless on driverless cars : #healthinsurance http://t.co/YdEtWgRibk',\n",
       " 'I rate Hazard very highly but his fanboys are among the worst accounts on Twitter.',\n",
       " 'Casualty Roleplay somebody please am so bored',\n",
       " 'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt',\n",
       " 'Alton brown just did a livestream and he burned the butter and touched the hot plate too soon and made a nut joke http://t.co/gvd7fcx8iZ',\n",
       " 'sevenfigz has a crush: http://t.co/20B3PnQxMD',\n",
       " 'Back on the beach after the deluge.  Surf camp in motion.  Our Surf Therapy programme kicked off today for... http://t.co/vjsAqPxngN',\n",
       " 'Why #Marijuana Is Critical For Research in Treating #PTSD\\n\\nhttp://t.co/T6fuAhFp7p \\n#hempoil #cannabis #marijuana\\x89Û_ http://t.co/RhE7dXM7Ey',\n",
       " 'Suicide bomber kills 15 in Saudi security site mosque - Reuters http://t.co/37DqvJHNCv',\n",
       " 'Cyclist who collided with runner on Roanoke greenway wins $300000 civil verdict - Roanoke Times: Cyclist who c... http://t.co/E2WfGp8JHk',\n",
       " \"So yeah splatoon is still lots of fun and default splattershot jr is still the only weapon layout I'm good at\",\n",
       " 'Had an awesome time gettin wrecked at bowling last night! http://t.co/Da9lZtOn1c',\n",
       " \"Wreckage 'Conclusively Confirmed' as From MH370: Malaysia PM: Investigators and the families of those who were... http://t.co/nn6Y0fD3l0\",\n",
       " '@TrustyMcLusty no passengers were on the derailed train. But the who morning commute is fcked. Go home.',\n",
       " 'Am I hearing thunder or trucks?',\n",
       " 'Selection September Bridgeport men charged in home invasion Police looking for burglar. - http://t.co/7mlCD0l0b8',\n",
       " 'To fight bioterrorism sir.',\n",
       " 'Towboat trek sympathy deluge falls: VTc http://t.co/eaaQUMkkc9']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting 30 random from text column randomly\n",
    "random.sample(list((train_df.text)),k=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data incosistencies or redundant information found in the dataset are as follows\n",
    "\n",
    "* Upper case and lower case at unexpected location\n",
    "* Punctuations\n",
    "* Numbers in text \n",
    "* Use of cities, states and Country names. (Granularity problem)\n",
    "* Special characters such as \\x89ÛÒ and \\n\n",
    "* Hyperklinks\n",
    "* Tags in tweets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'location' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* Removes texts with numbers\n",
    "* Removes cities names if country/state names are mentioned. (High level granularity is maintained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to chaange text to lower case and remove punctionation\n",
    "def cleaning_location(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    #lower case\n",
    "    text = text.split(',')[-1:][0].strip() # Removing city names when country/state name is present\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  #removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) #removing text with number\n",
    "    return text\n",
    "\n",
    "# Applying the method to location column\n",
    "train_df.location = train_df.location.apply(cleaning_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below does the following to clean anomalies in 'keyword' column:\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes punctutations\n",
    "* replaces number with space (as %20 was found in middle of two words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_keyword(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('(\\d+)', ' ', text)   #replacing numbers with space\n",
    "    return text\n",
    "\n",
    "# Applying the method to keyword column\n",
    "train_df.keyword = train_df.keyword.apply(cleaning_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method to clean anomalies in 'text' column does the following\n",
    "\n",
    "* Changes all the text to lower case\n",
    "* Removes words starting with @ to remove the tags and mentions example: @barackobama\n",
    "* Adds a column with hashtag values\n",
    "* Removes links\n",
    "* Removes punctuation\n",
    "* Removes words with numbers\n",
    "* Removes special characters examples: \\x89û,\\x89ûó etc \n",
    "* Removes '\\n' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(text):\n",
    "    \n",
    "    text = str(text)\n",
    "    text = text.lower()    ##lower case\n",
    "    \n",
    "    text = re.sub(r'@[A-Za-z]+[A-Za-z0-9-_]+', '',text) #removing any word starting with @   \\w\n",
    "    text = re.sub(r'https|www|http\\S+', '', text)  #removing any word starting with http\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  ##removing punctuations\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  #removing words with numbers\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) # removing special characters\n",
    "    text.replace(\"\\n\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method converts NLTK tags to wordnet tags which would be used to lemmatize the words in the following method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method is used to lemmatize the sentences in the following order:\n",
    "\n",
    "* Clean the sentences by calling the cleaning_text method on each sentence.\n",
    "* Tokenizing the sentence and generating a nltk POS tag for each word in the cleaned sentence.\n",
    "* Converting the nltk POS tag to Wordnet POS tag by calling wordnet_tag method.\n",
    "* Removes the stopwords\n",
    "* Lemmatizes the tokens using the pos tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence = cleaning_text(sentence) #Cleaning the sentence\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  #Tokenizing and tagging each word \n",
    "    wordnet_tagged = map(lambda x: (x[0], wordnet_tag(x[1])), nltk_tagged)  # Coverting the NLTK tags to wordnet tag\n",
    "    \n",
    "    #Lemmatizing the tagged tokens\n",
    "    lemmatized_sentence = [] #empty list for lemmatized words\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if word not in set(stopwords.words('english')):  #removing stopwords\n",
    "            if tag is None:                   \n",
    "                lemmatized_sentence.append(word) #adding the word as it is if POS tag missing\n",
    "            else:        \n",
    "                #else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizing.lemmatize(word, tag))  ##lemmatizing the token using the POS tag\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the lemmatizing method to text column \n",
    "train_df.text= train_df.text.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3456</td>\n",
       "      <td>4944</td>\n",
       "      <td>exploded</td>\n",
       "      <td>jamaica</td>\n",
       "      <td>go replace sarcasm meter explode</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>4934</td>\n",
       "      <td>exploded</td>\n",
       "      <td>usa</td>\n",
       "      <td>tell ten year ago id see anime big screen prob...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>5189</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>france</td>\n",
       "      <td>estimate damage fatality hiroshimasized atomic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1577</td>\n",
       "      <td>2278</td>\n",
       "      <td>cliff fall</td>\n",
       "      <td>bc</td>\n",
       "      <td>dont let style fall flat summer lord amp cliff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6236</td>\n",
       "      <td>8905</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>uk</td>\n",
       "      <td>new photo oak snowstorm take winter southdowns...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7110</td>\n",
       "      <td>10187</td>\n",
       "      <td>violent storm</td>\n",
       "      <td>costa rica</td>\n",
       "      <td>rt jupiter red spot violent storm large entire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>1425</td>\n",
       "      <td>body bag</td>\n",
       "      <td>nan</td>\n",
       "      <td>im im say theyre lucky go home family put body...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830</td>\n",
       "      <td>8329</td>\n",
       "      <td>rubble</td>\n",
       "      <td>west africa</td>\n",
       "      <td>tnn china stock market crash gem rubble</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5888</td>\n",
       "      <td>8409</td>\n",
       "      <td>sandstorm</td>\n",
       "      <td>united states</td>\n",
       "      <td>watch airport get swallow sandstorm minute</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>1852</td>\n",
       "      <td>burned</td>\n",
       "      <td>dc</td>\n",
       "      <td>burn calorie minute walk mph brisk pace myfitn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6594</td>\n",
       "      <td>9445</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>nan</td>\n",
       "      <td>truth news bbc cnn islam truth god isi terrori...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>1235</td>\n",
       "      <td>blood</td>\n",
       "      <td>nan</td>\n",
       "      <td>star reviewer dragon blood boxset lindsay buro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1217</td>\n",
       "      <td>1755</td>\n",
       "      <td>buildings burning</td>\n",
       "      <td>nan</td>\n",
       "      <td>bradford back best burn building read weep leeds</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1554</td>\n",
       "      <td>bomb</td>\n",
       "      <td>nan</td>\n",
       "      <td>new document find point japan wwii atomic bomb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2263</td>\n",
       "      <td>3243</td>\n",
       "      <td>deluged</td>\n",
       "      <td>nan</td>\n",
       "      <td>business deluge invoice make stand ogt colomr ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1041</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>nan</td>\n",
       "      <td>apparently youre bleed people look weird lol w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>912</td>\n",
       "      <td>bioterrorism</td>\n",
       "      <td>nan</td>\n",
       "      <td>wugliness due ugly frat bioterrorismim whos id...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4703</td>\n",
       "      <td>6687</td>\n",
       "      <td>landslide</td>\n",
       "      <td>the circle of life</td>\n",
       "      <td>youre catch landslide ill rain give sunshine ill</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>1760</td>\n",
       "      <td>buildings burning</td>\n",
       "      <td>dallas</td>\n",
       "      <td>like music video want real action shit like bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>806</td>\n",
       "      <td>battle</td>\n",
       "      <td>san francisco</td>\n",
       "      <td>really happen take king story trailer space ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id            keyword            location  \\\n",
       "3456   4944           exploded             jamaica   \n",
       "3450   4934           exploded                 usa   \n",
       "3640   5189         fatalities              france   \n",
       "1577   2278         cliff fall                  bc   \n",
       "6236   8905          snowstorm                  uk   \n",
       "7110  10187      violent storm          costa rica   \n",
       "984    1425           body bag                 nan   \n",
       "5830   8329             rubble         west africa   \n",
       "5888   8409          sandstorm       united states   \n",
       "1284   1852             burned                  dc   \n",
       "6594   9445          terrorism                 nan   \n",
       "853    1235              blood                 nan   \n",
       "1217   1755  buildings burning                 nan   \n",
       "1075   1554               bomb                 nan   \n",
       "2263   3243            deluged                 nan   \n",
       "720    1041           bleeding                 nan   \n",
       "632     912       bioterrorism                 nan   \n",
       "4703   6687          landslide  the circle of life   \n",
       "1221   1760  buildings burning              dallas   \n",
       "557     806             battle       san francisco   \n",
       "\n",
       "                                                   text  target  \n",
       "3456                   go replace sarcasm meter explode       0  \n",
       "3450  tell ten year ago id see anime big screen prob...       0  \n",
       "3640  estimate damage fatality hiroshimasized atomic...       1  \n",
       "1577  dont let style fall flat summer lord amp cliff...       0  \n",
       "6236  new photo oak snowstorm take winter southdowns...       1  \n",
       "7110  rt jupiter red spot violent storm large entire...       0  \n",
       "984   im im say theyre lucky go home family put body...       0  \n",
       "5830            tnn china stock market crash gem rubble       1  \n",
       "5888         watch airport get swallow sandstorm minute       1  \n",
       "1284  burn calorie minute walk mph brisk pace myfitn...       0  \n",
       "6594  truth news bbc cnn islam truth god isi terrori...       0  \n",
       "853   star reviewer dragon blood boxset lindsay buro...       0  \n",
       "1217   bradford back best burn building read weep leeds       1  \n",
       "1075  new document find point japan wwii atomic bomb...       1  \n",
       "2263  business deluge invoice make stand ogt colomr ...       0  \n",
       "720   apparently youre bleed people look weird lol w...       0  \n",
       "632   wugliness due ugly frat bioterrorismim whos id...       1  \n",
       "4703   youre catch landslide ill rain give sunshine ill       0  \n",
       "1221  like music video want real action shit like bu...       1  \n",
       "557   really happen take king story trailer space ba...       0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if text from any row was completely removed due to cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5115</td>\n",
       "      <td>7295</td>\n",
       "      <td>nuclear reactor</td>\n",
       "      <td>nan</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id          keyword location text  target\n",
       "5115  7295  nuclear reactor      nan            0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df.text == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one row with id 7295 where the whole text was removed. \n",
    "\n",
    "The original train data is read again to see the content of the row with id 7295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5115</td>\n",
       "      <td>7295</td>\n",
       "      <td>nuclear%20reactor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Err:509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            keyword location     text  target\n",
       "5115  7295  nuclear%20reactor      NaN  Err:509       0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crosscheck = pd.read_csv('train.csv')\n",
    "df_crosscheck.loc[df_crosscheck.id == 7295]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed above that the text in that particular row is an error message. Hence, that text was correctly cleaned from that row. \n",
    "\n",
    "The row is removed from the dataframe and the dataframe is saved as a csv file to be used for EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7612 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      "id          7612 non-null int64\n",
      "keyword     7612 non-null object\n",
      "location    7612 non-null object\n",
      "text        7612 non-null object\n",
      "target      7612 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 356.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df[train_df.text != '']\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"Clean_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning test data using the predefined methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the method to location column\n",
    "test_df.location = test_df.location.apply(cleaning_location)\n",
    "# Applying the method to keyword column\n",
    "test_df.keyword = test_df.keyword.apply(cleaning_keyword)\n",
    "\n",
    "# Applying the lemmatizing method to text column \n",
    "test_df.text= test_df.text.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('Clean_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
